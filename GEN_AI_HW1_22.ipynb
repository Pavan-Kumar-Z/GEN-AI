{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOK9FpUU+K8RLmaDRs4/QkU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qikdf0LCOyp9","executionInfo":{"status":"ok","timestamp":1739650421981,"user_tz":-330,"elapsed":650,"user":{"displayName":"Pavan Kumar","userId":"01467024048724399068"}},"outputId":"ed4ede11-8774-4bad-818c-2d74ae462cf5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7a4610aa2570>"]},"metadata":{},"execution_count":10}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","\n","torch.manual_seed(42)\n"]},{"cell_type":"code","source":["def generate_data(num_samples=1000):\n","    X = torch.randn(num_samples, 1)\n","    y = 3 * X + torch.randn(num_samples, 1)\n","    return X, y\n","\n","class DeepNet(nn.Module):\n","    def __init__(self, use_batchnorm=False, init_scale=1.0):\n","        super(DeepNet, self).__init__()\n","        self.fc1 = nn.Linear(1, 128)\n","        self.fc2 = nn.Linear(128, 128)\n","        self.fc3 = nn.Linear(128, 128)\n","        self.fc4 = nn.Linear(128, 1)\n","        self.relu = nn.ReLU()\n","        self.use_batchnorm = use_batchnorm\n","        if use_batchnorm:\n","            self.bn1 = nn.BatchNorm1d(128)\n","            self.bn2 = nn.BatchNorm1d(128)\n","            self.bn3 = nn.BatchNorm1d(128)\n","\n","        # Initialize weights\n","        self._initialize_weights(init_scale)\n","\n","    def _initialize_weights(self, init_scale):\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear):\n","                nn.init.normal_(m.weight, mean=0.0, std=init_scale)\n","                nn.init.zeros_(m.bias)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        if self.use_batchnorm:\n","            x = self.bn1(x)\n","        x = self.relu(x)\n","\n","        x = self.fc2(x)\n","        if self.use_batchnorm:\n","            x = self.bn2(x)\n","        x = self.relu(x)\n","\n","        x = self.fc3(x)\n","        if self.use_batchnorm:\n","            x = self.bn3(x)\n","        x = self.relu(x)\n","\n","        x = self.fc4(x)\n","        return x\n"],"metadata":{"id":"mgqbDR_sO9L7","executionInfo":{"status":"ok","timestamp":1739650421981,"user_tz":-330,"elapsed":4,"user":{"displayName":"Pavan Kumar","userId":"01467024048724399068"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def train_model(model, X, y, num_epochs=2000, lr=0.003):\n","    criterion = nn.MSELoss()\n","    optimizer = optim.SGD(model.parameters(), lr=lr)\n","    losses = []\n","\n","    for epoch in range(num_epochs):\n","        outputs = model(X)\n","        loss = criterion(outputs, y)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        losses.append(loss.item())\n","\n","        if (epoch + 1) % 200 == 0:\n","            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n","\n","    return losses\n","\n"],"metadata":{"id":"YnQq2fOjPDlT","executionInfo":{"status":"ok","timestamp":1739650421981,"user_tz":-330,"elapsed":3,"user":{"displayName":"Pavan Kumar","userId":"01467024048724399068"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["X, y = generate_data()\n","\n","# Experiment 1: Large weight initialization\n","print(\"Training with large weight initialization...\")\n","model_large = DeepNet(use_batchnorm=False, init_scale=10.0)\n","losses_large = train_model(model_large, X, y)\n","\n","# Experiment 2: Small weight initialization\n","print(\"\\nTraining with small weight initialization...\")\n","model_small = DeepNet(use_batchnorm=False, init_scale=0.01)\n","losses_small = train_model(model_small, X, y)\n","\n","# Experiment 3: Batch Normalization\n","print(\"\\nTraining with Batch Normalization...\")\n","model_bn = DeepNet(use_batchnorm=True, init_scale=1.0)\n","losses_bn = train_model(model_bn, X, y)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KVTauL0KPG16","executionInfo":{"status":"ok","timestamp":1739650467072,"user_tz":-330,"elapsed":45094,"user":{"displayName":"Pavan Kumar","userId":"01467024048724399068"}},"outputId":"d67f0579-ed8a-4c9a-9792-9e6d4fce19d8"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Training with large weight initialization...\n","Epoch [200/2000], Loss: nan\n","Epoch [400/2000], Loss: nan\n","Epoch [600/2000], Loss: nan\n","Epoch [800/2000], Loss: nan\n","Epoch [1000/2000], Loss: nan\n","Epoch [1200/2000], Loss: nan\n","Epoch [1400/2000], Loss: nan\n","Epoch [1600/2000], Loss: nan\n","Epoch [1800/2000], Loss: nan\n","Epoch [2000/2000], Loss: nan\n","\n","Training with small weight initialization...\n","Epoch [200/2000], Loss: 10.2456\n","Epoch [400/2000], Loss: 10.2455\n","Epoch [600/2000], Loss: 10.2454\n","Epoch [800/2000], Loss: 10.2454\n","Epoch [1000/2000], Loss: 10.2454\n","Epoch [1200/2000], Loss: 10.2454\n","Epoch [1400/2000], Loss: 10.2453\n","Epoch [1600/2000], Loss: 10.2453\n","Epoch [1800/2000], Loss: 10.2452\n","Epoch [2000/2000], Loss: 10.2452\n","\n","Training with Batch Normalization...\n","Epoch [200/2000], Loss: 1.0121\n","Epoch [400/2000], Loss: 1.0050\n","Epoch [600/2000], Loss: 1.0016\n","Epoch [800/2000], Loss: 0.9983\n","Epoch [1000/2000], Loss: 0.9974\n","Epoch [1200/2000], Loss: 0.9959\n","Epoch [1400/2000], Loss: 0.9969\n","Epoch [1600/2000], Loss: 0.9935\n","Epoch [1800/2000], Loss: 0.9977\n","Epoch [2000/2000], Loss: 0.9940\n"]}]},{"cell_type":"markdown","source":["We see the gradients explode resulting in loss divergence in the case of large weight initialization"],"metadata":{"id":"F6ijX2RzP-Ph"}},{"cell_type":"code","source":[],"metadata":{"id":"NgNGMWnYP7pI"},"execution_count":null,"outputs":[]}]}