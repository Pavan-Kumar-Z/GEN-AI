{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMocuV63sVfhBX/OkujrCut"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"U9OpAYoMQBZf"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n"]},{"cell_type":"code","source":["def f(x, y):\n","    return x**2 + y**2\n","\n","def grad_f(x, y):\n","    return np.array([2 * x, 2 * y])\n","\n","# Standard Gradient Descent (GD)\n","def gradient_descent(x, y, learning_rate=0.1, epochs=100):\n","    history = []\n","    for _ in range(epochs):\n","        grad = grad_f(x, y)\n","        x -= learning_rate * grad[0]\n","        y -= learning_rate * grad[1]\n","        history.append((x, y, f(x, y)))\n","    return history\n","\n","# Stochastic Gradient Descent (SGD)\n","def stochastic_gradient_descent(x, y, learning_rate=0.1, epochs=100):\n","    history = []\n","    for _ in range(epochs):\n","        grad = grad_f(x, y) + np.random.normal(0, 0.1, size=2)\n","        x -= learning_rate * grad[0]\n","        y -= learning_rate * grad[1]\n","        history.append((x, y, f(x, y)))\n","    return history\n","\n","# Momentum-based Gradient Descent\n","def momentum_gradient_descent(x, y, learning_rate=0.1, momentum=0.9, epochs=100):\n","    history = []\n","    vx, vy = 0, 0\n","    for _ in range(epochs):\n","        grad = grad_f(x, y)\n","        vx = momentum * vx + learning_rate * grad[0]\n","        vy = momentum * vy + learning_rate * grad[1]\n","        x -= vx\n","        y -= vy\n","        history.append((x, y, f(x, y)))\n","    return history\n","\n","# Adam Optimizer\n","def adam_optimizer(x, y, learning_rate=0.1, beta1=0.9, beta2=0.999, epsilon=1e-8, epochs=100):\n","    history = []\n","    m_x, m_y = 0, 0  # First moment estimates\n","    v_x, v_y = 0, 0  # Second moment estimates\n","    for t in range(1, epochs + 1):\n","        grad = grad_f(x, y)\n","\n","        m_x = beta1 * m_x + (1 - beta1) * grad[0]\n","        m_y = beta1 * m_y + (1 - beta1) * grad[1]\n","\n","        v_x = beta2 * v_x + (1 - beta2) * grad[0]**2\n","        v_y = beta2 * v_y + (1 - beta2) * grad[1]**2\n","\n","        m_x_hat = m_x / (1 - beta1**t)\n","        m_y_hat = m_y / (1 - beta1**t)\n","        v_x_hat = v_x / (1 - beta2**t)\n","        v_y_hat = v_y / (1 - beta2**t)\n","\n","        x -= learning_rate * m_x_hat / (np.sqrt(v_x_hat) + epsilon)\n","        y -= learning_rate * m_y_hat / (np.sqrt(v_y_hat) + epsilon)\n","        history.append((x, y, f(x, y)))\n","    return history\n","\n"],"metadata":{"id":"iFYTVQcTQl1P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x, y = 5.0, 5.0  # Starting point\n","\n","gd_history = gradient_descent(x, y, learning_rate=0.1, epochs=100)\n","sgd_history = stochastic_gradient_descent(x, y, learning_rate=0.1, epochs=100)\n","momentum_history = momentum_gradient_descent(x, y, learning_rate=0.1, momentum=0.9, epochs=100)\n","adam_history = adam_optimizer(x, y, learning_rate=0.1, epochs=100)\n","\n","gd_loss = [f(x, y) for x, y, _ in gd_history]\n","sgd_loss = [f(x, y) for x, y, _ in sgd_history]\n","momentum_loss = [f(x, y) for x, y, _ in momentum_history]\n","adam_loss = [f(x, y) for x, y, _ in adam_history]\n","\n"],"metadata":{"id":"35rS79I_QpZ8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(gd_loss, label=\"Gradient Descent\")\n","plt.plot(sgd_loss, label=\"Stochastic Gradient Descent\")\n","plt.plot(momentum_loss, label=\"Momentum Gradient Descent\")\n","plt.plot(adam_loss, label=\"Adam Optimizer\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.title(\"Comparison of Gradient Descent Variants on f(x, y) = x^2 + y^2\")\n","plt.legend()\n","plt.show()"],"metadata":{"id":"G0cSuxXIQrfg"},"execution_count":null,"outputs":[]}]}